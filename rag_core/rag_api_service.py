"""
RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ - FastAPI å¾®æœåŠ¡ç‰ˆæœ¬
åŸºäº LlamaIndex + DeepSeek + æœ¬åœ° Embedding
æä¾› HTTP API æ¥å£ï¼Œæ”¯æŒæ™®é€šé—®ç­”ã€æµå¼è¾“å‡ºå’Œæ–‡æ¡£ä¸Šä¼ 
"""

import os
import asyncio
from typing import Optional, List, Dict, Any
from contextlib import asynccontextmanager

import tiktoken
from fastapi import FastAPI, HTTPException, UploadFile, File, BackgroundTasks
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    Settings,
    StorageContext,
    load_index_from_storage,
    PromptTemplate
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openai_like import OpenAILike
from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker
from llama_index.core.callbacks import CallbackManager, TokenCountingHandler
from llama_index.readers.file import PyMuPDFReader
from llama_index.core.chat_engine import CondensePlusContextChatEngine
from llama_index.core.memory import ChatMemoryBuffer
from sse_starlette.sse import EventSourceResponse

# ==================== é…ç½®åŒº ====================

class Config:
    """ç³»ç»Ÿé…ç½®ç±» - æ‰€æœ‰å¯è°ƒå‚æ•°é›†ä¸­ç®¡ç†"""

    # DeepSeek APIé…ç½®
    DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")
    DEEPSEEK_MODEL = "deepseek-chat"
    DEEPSEEK_API_BASE = "https://api.deepseek.com/v1"
    TEMPERATURE = 0.3  # æ§åˆ¶å›ç­”çš„åˆ›é€ æ€§(0-1ï¼Œè¶Šé«˜è¶Šéšæœº)

    # æœ¬åœ°åµŒå…¥æ¨¡å‹é…ç½®
    EMBED_MODEL = "BAAI/bge-base-zh-v1.5"  # ä¸­æ–‡å‘é‡æ¨¡å‹
    RERANKER_MODEL = "BAAI/bge-reranker-base"  # é‡æ’åºæ¨¡å‹

    # æ–‡æœ¬åˆ†å—å‚æ•°
    CHUNK_SIZE = 512  # æ¯ä¸ªæ–‡æœ¬å—çš„å¤§å°
    CHUNK_OVERLAP = 50  # æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°

    # æ£€ç´¢å‚æ•°
    TOP_K = 10  # åˆæ­¥æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
    RERANK_TOP_N = 5  # é‡æ’åºåä¿ç•™çš„æ–‡æ¡£æ•°é‡

    # æˆæœ¬è®¡ç®—(DeepSeekå®˜æ–¹ä»·æ ¼ USD/ç™¾ä¸‡tokens)
    PRICE_INPUT = 0.14 / 1_000_000  # è¾“å…¥ä»·æ ¼
    PRICE_OUTPUT = 0.28 / 1_000_000  # è¾“å‡ºä»·æ ¼

    # è·¯å¾„é…ç½®
    DATA_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data")
    STORAGE_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), "storage")


# ==================== æç¤ºè¯æ¨¡æ¿ ====================

QA_PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä½æ·±æ€ç†Ÿè™‘çš„ç³»ç»Ÿæ¶æ„åˆ†æä¸“å®¶ã€‚ä¸‹é¢æ˜¯ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µï¼š

---------------------
{context_str}
---------------------

è¯·åŸºäºæä¾›çš„å‚è€ƒå†…å®¹å›ç­”é—®é¢˜ï¼š{query_str}

è¦æ±‚ï¼š
1. ä¸è¦ç®€å•å¤è¯»åŸæ–‡ï¼Œè¦æ ¹æ®ä¸Šä¸‹æ–‡è¿›è¡Œåˆç†è§£è¯»ï¼Œå¹¶ç»™å‡ºä¸“ä¸šå»ºè®®ã€‚
2. å¦‚æœåŸæ–‡æ²¡æœ‰ç›´è¯´ï¼Œè¯·ç»“åˆè¯­å¢ƒæ¨æ–­ã€‚
3. å¦‚æœå‚è€ƒå†…å®¹å®Œå…¨ä¸ç›¸å…³ï¼Œè¯·è¯šå®è¯´æ˜
4. å›ç­”è¦æ¡ç†æ¸…æ™°ï¼Œæ˜“äºç†è§£

å›ç­”ï¼š"""


# ==================== å•ä¾‹ç®¡ç†å™¨ ====================

class RAGServiceManager:
    """RAGæœåŠ¡ç®¡ç†å™¨ - å•ä¾‹æ¨¡å¼ç®¡ç†ç´¢å¼•å’Œå¼•æ“"""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(RAGServiceManager, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not self._initialized:
            self.token_counter = None
            self.index = None
            self.query_engine = None
            self.chat_engines = {}  # æŒ‰ä¼šè¯IDå­˜å‚¨èŠå¤©å¼•æ“
            self._initialized = True
    
    def init_settings(self):
        """åˆå§‹åŒ–å…¨å±€è®¾ç½®ï¼šLLMã€Embeddingã€Tokenè®¡æ•°å™¨"""
        
        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿ...")
        
        # 1. é…ç½®Tokenè®¡æ•°å™¨(ç”¨äºè¿½è¸ªæˆæœ¬)
        token_counter = TokenCountingHandler(
            tokenizer=tiktoken.get_encoding("cl100k_base").encode
        )
        Settings.callback_manager = CallbackManager([token_counter])
        
        # 2. é…ç½®äº‘ç«¯å¤§è¯­è¨€æ¨¡å‹(DeepSeek)
        if not Config.DEEPSEEK_API_KEY:
            raise ValueError("âŒ è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY")
        
        Settings.llm = OpenAILike(
            model=Config.DEEPSEEK_MODEL,
            api_key=Config.DEEPSEEK_API_KEY,
            api_base=Config.DEEPSEEK_API_BASE,
            temperature=Config.TEMPERATURE,
            is_chat_model=True,
            timeout=120.0
        )
        
        # 3. é…ç½®æœ¬åœ°åµŒå…¥æ¨¡å‹(ç”¨äºå‘é‡åŒ–æ–‡æœ¬)
        Settings.embed_model = HuggingFaceEmbedding(
            model_name=Config.EMBED_MODEL,
            embed_batch_size=40,
            device="cpu"
        )
        Settings.chunk_size = Config.CHUNK_SIZE
        Settings.chunk_overlap = Config.CHUNK_OVERLAP
        
        self.token_counter = token_counter
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\n")
    
    def get_or_create_index(self):
        """è·å–æˆ–åˆ›å»ºå‘é‡ç´¢å¼•"""
        
        if self.index is not None:
            return self.index
        
        if os.path.exists(Config.STORAGE_PATH):
            print(f"ğŸ“‚ å‘ç°å·²æœ‰ç´¢å¼•ï¼Œä» {Config.STORAGE_PATH} åŠ è½½...")
            storage_context = StorageContext.from_defaults(persist_dir=Config.STORAGE_PATH)
            index = load_index_from_storage(storage_context)
            print("âœ… ç´¢å¼•åŠ è½½æˆåŠŸ")
        else:
            print(f"ğŸ“š æœªæ‰¾åˆ°ç´¢å¼•ï¼Œå¼€å§‹æ„å»ºæ–°ç´¢å¼•...")
            print(f"ğŸ“– è¯»å–æ–‡æ¡£ï¼š{Config.DATA_PATH}")
            
            # é«˜é€Ÿè¯»å–
            reader = SimpleDirectoryReader(
                input_dir=Config.DATA_PATH,
                file_extractor={".pdf": PyMuPDFReader()}
            )
            documents = reader.load_data()
            
            print(f"ğŸ”¨ æ­£åœ¨æ„å»ºç´¢å¼•ï¼Œå¼€å¯å¤šæ ¸åŠ é€Ÿ...")
            index = VectorStoreIndex.from_documents(
                documents,
                show_progress=True,
                num_workers=4
            )
            
            index.storage_context.persist(persist_dir=Config.STORAGE_PATH)
            print("âœ… ç´¢å¼•æ„å»ºå®Œæˆ")
        
        self.index = index
        return index
    
    def create_query_engine(self):
        """åˆ›å»ºé…ç½®å¥½çš„æŸ¥è¯¢å¼•æ“"""
        
        if self.query_engine is not None:
            return self.query_engine
        
        print("âš™ï¸  é…ç½®æŸ¥è¯¢å¼•æ“...")
        
        # åˆå§‹åŒ–é‡æ’åºå™¨(æé«˜æ£€ç´¢ç²¾åº¦)
        reranker = FlagEmbeddingReranker(
            model=Config.RERANKER_MODEL,
            top_n=Config.RERANK_TOP_N
        )
        
        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        query_engine = self.index.as_query_engine(
            similarity_top_k=Config.TOP_K,
            node_postprocessors=[reranker],
            text_qa_template=PromptTemplate(QA_PROMPT_TEMPLATE)
        )
        
        self.query_engine = query_engine
        print("âœ… æŸ¥è¯¢å¼•æ“å°±ç»ª\n")
        return query_engine
    
    def get_chat_engine(self, conversation_id: str = "default"):
        """è·å–æˆ–åˆ›å»ºèŠå¤©å¼•æ“ï¼ˆå¸¦è®°å¿†åŠŸèƒ½ï¼‰"""
        
        if conversation_id not in self.chat_engines:
            print(f"âš™ï¸  ä¸ºä¼šè¯ {conversation_id} é…ç½®å¯¹è¯å¼æŸ¥è¯¢å¼•æ“...")
            memory = ChatMemoryBuffer.from_defaults(token_limit=3000)
            
            reranker = FlagEmbeddingReranker(
                model=Config.RERANKER_MODEL,
                top_n=Config.RERANK_TOP_N
            )
            
            chat_engine = self.index.as_chat_engine(
                chat_mode="condense_plus_context",
                streaming=True,
                memory=memory,
                similarity_top_k=3,
                system_prompt="ä½ æ˜¯ä¸€ä½é‡‘è/æ”¿åŠ¡é›†æˆæ¶æ„ä¸“å®¶...",
                context_prompt=QA_PROMPT_TEMPLATE
            )
            
            self.chat_engines[conversation_id] = chat_engine
            print(f"âœ… ä¼šè¯ {conversation_id} çš„å¯¹è¯å¼•æ“å°±ç»ª\n")
        
        return self.chat_engines[conversation_id]
    
    def reset_token_counter(self):
        """é‡ç½®Tokenè®¡æ•°å™¨"""
        if self.token_counter:
            self.token_counter.reset_counts()
    
    def get_token_stats(self) -> Dict[str, Any]:
        """è·å–Tokenæ¶ˆè€—ç»Ÿè®¡"""
        if not self.token_counter:
            return {}
        
        prompt_tokens = self.token_counter.prompt_llm_token_count
        completion_tokens = self.token_counter.completion_llm_token_count
        total_cost = (
            prompt_tokens * Config.PRICE_INPUT +
            completion_tokens * Config.PRICE_OUTPUT
        )
        
        return {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": prompt_tokens + completion_tokens,
            "total_cost_usd": total_cost,
            "input_price_per_token": Config.PRICE_INPUT,
            "output_price_per_token": Config.PRICE_OUTPUT
        }
    
    def add_document(self, file_path: str):
        """æ·»åŠ æ–°æ–‡æ¡£åˆ°ç´¢å¼•"""
        print(f"ğŸ“„ æ­£åœ¨æ·»åŠ æ–‡æ¡£: {file_path}")
        
        # è¯»å–æ–°æ–‡æ¡£
        reader = SimpleDirectoryReader(
            input_files=[file_path],
            file_extractor={".pdf": PyMuPDFReader()}
        )
        documents = reader.load_data()
        
        # å°†æ–°æ–‡æ¡£æ’å…¥ç´¢å¼•
        for doc in documents:
            self.index.insert(doc)
        
        # ä¿å­˜æ›´æ–°åçš„ç´¢å¼•
        self.index.storage_context.persist(persist_dir=Config.STORAGE_PATH)
        print(f"âœ… æ–‡æ¡£å·²æˆåŠŸæ·»åŠ åˆ°ç´¢å¼•å¹¶ä¿å­˜")


# ==================== Pydantic æ¨¡å‹ ====================

class QueryRequest(BaseModel):
    """æŸ¥è¯¢è¯·æ±‚æ¨¡å‹"""
    query: str = Field(..., description="ç”¨æˆ·æŸ¥è¯¢é—®é¢˜")
    conversation_id: Optional[str] = Field("default", description="ä¼šè¯IDï¼Œç”¨äºå¤šè½®å¯¹è¯")
    user_id: Optional[str] = Field(None, description="ç”¨æˆ·IDï¼Œç”¨äºç»Ÿè®¡å’Œä¸ªæ€§åŒ–")

class QueryResponse(BaseModel):
    """æŸ¥è¯¢å“åº”æ¨¡å‹"""
    answer: str = Field(..., description="AIå›ç­”å†…å®¹")
    conversation_id: str = Field(..., description="ä¼šè¯ID")
    token_stats: Optional[Dict[str, Any]] = Field(None, description="Tokenæ¶ˆè€—ç»Ÿè®¡")
    sources: Optional[List[Dict[str, Any]]] = Field(None, description="æ¥æºæ–‡æ¡£ä¿¡æ¯")

class UploadResponse(BaseModel):
    """ä¸Šä¼ å“åº”æ¨¡å‹"""
    success: bool = Field(..., description="ä¸Šä¼ æ˜¯å¦æˆåŠŸ")
    message: str = Field(..., description="å“åº”æ¶ˆæ¯")
    file_path: Optional[str] = Field(None, description="ä¿å­˜çš„æ–‡ä»¶è·¯å¾„")
    document_count: Optional[int] = Field(None, description="å¤„ç†çš„æ–‡æ¡£æ•°é‡")

class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”æ¨¡å‹"""
    status: str = Field(..., description="æœåŠ¡çŠ¶æ€")
    index_loaded: bool = Field(..., description="ç´¢å¼•æ˜¯å¦åŠ è½½")
    model_ready: bool = Field(..., description="æ¨¡å‹æ˜¯å¦å°±ç»ª")
    storage_path: str = Field(..., description="å­˜å‚¨è·¯å¾„")


# ==================== FastAPI åº”ç”¨ ====================

# åˆ›å»ºå•ä¾‹ç®¡ç†å™¨
rag_manager = RAGServiceManager()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    print("ğŸš€ RAG API æœåŠ¡å¯åŠ¨ä¸­...")
    try:
        rag_manager.init_settings()
        rag_manager.get_or_create_index()
        rag_manager.create_query_engine()
        print("âœ… RAG API æœåŠ¡å¯åŠ¨å®Œæˆ")
    except Exception as e:
        print(f"âŒ æœåŠ¡å¯åŠ¨å¤±è´¥: {str(e)}")
        raise
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†
    print("ğŸ›‘ RAG API æœåŠ¡å…³é—­ä¸­...")

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="RAGæ™ºèƒ½é—®ç­”API",
    description="åŸºäºLlamaIndexå’ŒDeepSeekçš„RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ",
    version="1.0.0",
    lifespan=lifespan
)


# ==================== API æ¥å£ ====================

@app.get("/")
async def root():
    """æ ¹è·¯å¾„ï¼Œè¿”å›æœåŠ¡ä¿¡æ¯"""
    return {
        "service": "RAGæ™ºèƒ½é—®ç­”API",
        "version": "1.0.0",
        "endpoints": {
            "POST /query": "æ™®é€šé—®ç­”æ¥å£",
            "POST /query_stream": "æµå¼è¾“å‡ºæ¥å£",
            "POST /upload_doc": "ä¸Šä¼ æ–‡æ¡£æ¥å£",
            "GET /health": "å¥åº·æ£€æŸ¥æ¥å£"
        }
    }

@app.get("/health")
async def health_check() -> HealthResponse:
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return HealthResponse(
        status="healthy",
        index_loaded=rag_manager.index is not None,
        model_ready=rag_manager.query_engine is not None,
        storage_path=Config.STORAGE_PATH
    )

@app.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """æ™®é€šé—®ç­”æ¥å£"""
    try:
        # é‡ç½®Tokenè®¡æ•°å™¨
        rag_manager.reset_token_counter()
        
        # è·å–èŠå¤©å¼•æ“
        chat_engine = rag_manager.get_chat_engine(request.conversation_id)
        
        print(f"ğŸ” æ­£åœ¨å¤„ç†æŸ¥è¯¢: {request.query[:50]}...")
        
        # æ‰§è¡ŒæŸ¥è¯¢
        response = await asyncio.to_thread(
            chat_engine.chat,
            request.query
        )
        
        # è·å–Tokenç»Ÿè®¡
        token_stats = rag_manager.get_token_stats()
        
        # æå–æ¥æºä¿¡æ¯
        sources = []
        if hasattr(response, 'source_nodes'):
            for i, node in enumerate(response.source_nodes, 1):
                sources.append({
                    "index": i,
                    "score": float(node.score) if node.score else 0.0,
                    "content_preview": node.node.get_content()[:100].replace('\n', ' ')
                })
        
        return QueryResponse(
            answer=str(response),
            conversation_id=request.conversation_id,
            token_stats=token_stats,
            sources=sources if sources else None
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")

@app.post("/query_stream")
async def query_stream(request: QueryRequest):
    """æµå¼è¾“å‡ºæ¥å£"""
    
    async def event_generator():
        """äº‹ä»¶ç”Ÿæˆå™¨ï¼Œç”¨äºæµå¼è¾“å‡º"""
        try:
            # è·å–èŠå¤©å¼•æ“
            chat_engine = rag_manager.get_chat_engine(request.conversation_id)
            
            print(f"ğŸ” æ­£åœ¨å¤„ç†æµå¼æŸ¥è¯¢: {request.query[:50]}...")
            
            # æ‰§è¡Œæµå¼æŸ¥è¯¢
            streaming_response = chat_engine.stream_chat(request.query)
            
            # æµå¼è¾“å‡ºå›ç­”
            for token in streaming_response.response_gen:
                yield {
                    "event": "message",
                    "data": token
                }
                await asyncio.sleep(0.01)  # å°å»¶è¿Ÿï¼Œé¿å…å‘é€è¿‡å¿«
            
            # å‘é€å®Œæˆäº‹ä»¶
            yield {
                "event": "complete",
                "data": "stream_completed"
            }
            
        except Exception as e:
            yield {
                "event": "error",
                "data": f"æµå¼æŸ¥è¯¢å¤±è´¥: {str(e)}"
            }
    
    return EventSourceResponse(event_generator())

@app.post("/upload_doc", response_model=UploadResponse)
async def upload_doc(
    file: UploadFile = File(...),
    background_tasks: BackgroundTasks = None
):
    """ä¸Šä¼ æ–‡æ¡£æ¥å£"""
    try:
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if not file.filename.endswith('.pdf'):
            return UploadResponse(
                success=False,
                message="ä»…æ”¯æŒPDFæ–‡ä»¶æ ¼å¼"
            )
        
        # åˆ›å»ºä¸´æ—¶ä¿å­˜è·¯å¾„
        upload_dir = os.path.join(Config.DATA_PATH, "uploads")
        os.makedirs(upload_dir, exist_ok=True)
        
        file_path = os.path.join(upload_dir, file.filename)
        
        # ä¿å­˜æ–‡ä»¶
        content = await file.read()
        with open(file_path, "wb") as f:
            f.write(content)
        
        # åœ¨åå°ä»»åŠ¡ä¸­æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•
        if background_tasks:
            background_tasks.add_task(rag_manager.add_document, file_path)
            return UploadResponse(
                success=True,
                message="æ–‡ä»¶å·²ä¸Šä¼ ï¼Œæ­£åœ¨åå°å¤„ç†æ·»åŠ åˆ°ç´¢å¼•",
                file_path=file_path,
                document_count=1
            )
        else:
            # å¦‚æœæ²¡æœ‰åå°ä»»åŠ¡ï¼Œç›´æ¥å¤„ç†
            rag_manager.add_document(file_path)
            return UploadResponse(
                success=True,
                message="æ–‡ä»¶å·²ä¸Šä¼ å¹¶æˆåŠŸæ·»åŠ åˆ°ç´¢å¼•",
                file_path=file_path,
                document_count=1
            )
        
    except Exception as e:
        return UploadResponse(
            success=False,
            message=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}"
        )


# ==================== ä¸»ç¨‹åºå…¥å£ ====================

if __name__ == "__main__":
    import uvicorn
    
    print("ğŸš€ å¯åŠ¨ RAG API æœåŠ¡...")
    print(f"ğŸ“‚ æ•°æ®è·¯å¾„: {Config.DATA_PATH}")
    print(f"ğŸ’¾ å­˜å‚¨è·¯å¾„: {Config.STORAGE_PATH}")
    
    uvicorn.run(
        "rag_api_service:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
