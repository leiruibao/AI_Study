"""
RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ - åŸºäºLlamaIndex + DeepSeek + æœ¬åœ°Embedding
åŠŸèƒ½ï¼šè¯»å–æœ¬åœ°æ–‡æ¡£ï¼Œæ„å»ºå‘é‡ç´¢å¼•ï¼Œæ”¯æŒæ™ºèƒ½é—®ç­”å’Œæˆæœ¬è¿½è¸ª
"""

import os
import shutil
from pathlib import Path
#os.environ["HF_HUB_OFFLINE"] = "1"
# 2. å…³é”®ï¼šæ”¹ç”¨å›½å†…é•œåƒç«™
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HOME"] = r"F:\AI_Models\huggingface"
from urllib import response
import tiktoken
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    Settings,
    StorageContext,
    load_index_from_storage,
    PromptTemplate
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openai_like import OpenAILike
from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker
from llama_index.core.callbacks import CallbackManager, TokenCountingHandler
from llama_index.readers.file import PyMuPDFReader
#from llama_index.core import Settings
from llama_index.core.chat_engine import CondensePlusContextChatEngine
from llama_index.core.memory import ChatMemoryBuffer



# ==================== é…ç½®åŒº ====================

class Config:
    """ç³»ç»Ÿé…ç½®ç±» - æ‰€æœ‰å¯è°ƒå‚æ•°é›†ä¸­ç®¡ç†"""

    # DeepSeek APIé…ç½®
    DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")
    DEEPSEEK_MODEL = "deepseek-chat"
    DEEPSEEK_API_BASE = "https://api.deepseek.com/v1"
    TEMPERATURE = 0.3  # æ§åˆ¶å›ç­”çš„åˆ›é€ æ€§(0-1ï¼Œè¶Šé«˜è¶Šéšæœº)

    # æœ¬åœ°åµŒå…¥æ¨¡å‹é…ç½®
    EMBED_MODEL = "BAAI/bge-base-zh-v1.5"  # ä¸­æ–‡å‘é‡æ¨¡å‹
    RERANKER_MODEL = "BAAI/bge-reranker-base"  # é‡æ’åºæ¨¡å‹

    # æ–‡æœ¬åˆ†å—å‚æ•°
    CHUNK_SIZE = 512  # æ¯ä¸ªæ–‡æœ¬å—çš„å¤§å°
    CHUNK_OVERLAP = 50  # æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°

    # æ£€ç´¢å‚æ•°
    TOP_K = 10  # åˆæ­¥æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
    RERANK_TOP_N = 5  # é‡æ’åºåä¿ç•™çš„æ–‡æ¡£æ•°é‡

    # æˆæœ¬è®¡ç®—(DeepSeekå®˜æ–¹ä»·æ ¼ USD/ç™¾ä¸‡tokens)
    PRICE_INPUT = 0.14 / 1_000_000  # è¾“å…¥ä»·æ ¼
    PRICE_OUTPUT = 0.28 / 1_000_000  # è¾“å‡ºä»·æ ¼


# ==================== æç¤ºè¯æ¨¡æ¿ ====================

QA_PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä½æ·±æ€ç†Ÿè™‘çš„ç³»ç»Ÿæ¶æ„åˆ†æä¸“å®¶ã€‚ä¸‹é¢æ˜¯ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µï¼š

---------------------
{context_str}
---------------------

è¯·åŸºäºæä¾›çš„å‚è€ƒå†…å®¹å›ç­”é—®é¢˜ï¼š{query_str}

è¦æ±‚ï¼š
1. ä¸è¦ç®€å•å¤è¯»åŸæ–‡ï¼Œè¦æ ¹æ®ä¸Šä¸‹æ–‡è¿›è¡Œåˆç†è§£è¯»ï¼Œå¹¶ç»™å‡ºä¸“ä¸šå»ºè®®ã€‚
2. å¦‚æœåŸæ–‡æ²¡æœ‰ç›´è¯´ï¼Œè¯·ç»“åˆè¯­å¢ƒæ¨æ–­ã€‚
3. å¦‚æœå‚è€ƒå†…å®¹å®Œå…¨ä¸ç›¸å…³ï¼Œè¯·è¯šå®è¯´æ˜
4. å›ç­”è¦æ¡ç†æ¸…æ™°ï¼Œæ˜“äºç†è§£

å›ç­”ï¼š"""


# ==================== ç³»ç»Ÿåˆå§‹åŒ– ====================

def init_settings():
    """åˆå§‹åŒ–å…¨å±€è®¾ç½®ï¼šLLMã€Embeddingã€Tokenè®¡æ•°å™¨"""

    print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿ...")

    # 1. é…ç½®Tokenè®¡æ•°å™¨(ç”¨äºè¿½è¸ªæˆæœ¬)
    token_counter = TokenCountingHandler(
        tokenizer=tiktoken.get_encoding("cl100k_base").encode
    )
    Settings.callback_manager = CallbackManager([token_counter])

    # 2. é…ç½®äº‘ç«¯å¤§è¯­è¨€æ¨¡å‹(DeepSeek)
    if not Config.DEEPSEEK_API_KEY:
        raise ValueError("âŒ è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY")

    Settings.llm = OpenAILike(
        model=Config.DEEPSEEK_MODEL,
        api_key=Config.DEEPSEEK_API_KEY,
        api_base=Config.DEEPSEEK_API_BASE,
        temperature=Config.TEMPERATURE,
        is_chat_model=True,
        timeout=120.0 # æ˜¾å¼è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º 120 ç§’
    )

    # 3. é…ç½®æœ¬åœ°åµŒå…¥æ¨¡å‹(ç”¨äºå‘é‡åŒ–æ–‡æœ¬)
    Settings.embed_model = HuggingFaceEmbedding(
        model_name=Config.EMBED_MODEL,
        embed_batch_size=40,  # CPU å»ºè®® 32-64ï¼Œæé«˜å‘é‡åŒ–é€Ÿåº¦
        device="cpu"
    )
    Settings.chunk_size = Config.CHUNK_SIZE
    Settings.chunk_overlap = Config.CHUNK_OVERLAP

    print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\n")
    return token_counter


# ==================== ç´¢å¼•ç®¡ç† ====================

def get_or_create_index(data_path, storage_path):
    """
    è·å–æˆ–åˆ›å»ºå‘é‡ç´¢å¼•
    - å¦‚æœç´¢å¼•å·²å­˜åœ¨ï¼Œç›´æ¥åŠ è½½
    - å¦‚æœä¸å­˜åœ¨ï¼Œä»æ–‡æ¡£åˆ›å»ºæ–°ç´¢å¼•
    """

    if os.path.exists(storage_path):
        print(f"ğŸ“‚ å‘ç°å·²æœ‰ç´¢å¼•ï¼Œä» {storage_path} åŠ è½½...")
        storage_context = StorageContext.from_defaults(persist_dir=storage_path)
        index = load_index_from_storage(storage_context)
        print("âœ… ç´¢å¼•åŠ è½½æˆåŠŸ")
    else:
        print(f"ğŸ“š æœªæ‰¾åˆ°ç´¢å¼•ï¼Œå¼€å§‹æ„å»ºæ–°ç´¢å¼•...")
        print(f"ğŸ“– è¯»å–æ–‡æ¡£ï¼š{data_path}")

        # 1. é«˜é€Ÿè¯»å–
        reader = SimpleDirectoryReader(
            input_dir=data_path,
            file_extractor={".pdf": PyMuPDFReader()} 
        )
        documents = reader.load_data()

        # # 2. è¯Šæ–­æ–‡æœ¬é‡
        # char_count = sum(len(d.text) for d in documents)
        # print(f"âœ… è§£æå®Œæˆï¼å…± {len(documents)} é¡µï¼Œæ€»å­—ç¬¦æ•°: {char_count:,}")

        # # 3. å¹¶è¡Œæ„å»ºï¼ˆå¢åŠ è¿›åº¦æ¡ï¼‰
        # print("ğŸ”¨ æ­£åœ¨å¹¶è¡Œæ„å»ºå‘é‡ç´¢å¼•ï¼ˆè¯·çœ‹è¿›åº¦æ¡ï¼‰...")
        # index = VectorStoreIndex.from_documents(
        #     documents,
        #     show_progress=True, # å¼ºçƒˆå»ºè®®å¼€å¯ï¼Œé¿å…ç„¦è™‘
        #     num_workers=8       # å……åˆ†åˆ©ç”¨ i7 çš„å¤šæ ¸
        # )

        print(f"ğŸ”¨ æ­£åœ¨æ„å»ºç´¢å¼•ï¼Œå¼€å¯å¤šæ ¸åŠ é€Ÿ...")
        # show_progress=True ä¼šæ˜¾ç¤ºè¿›åº¦æ¡ï¼Œä½ å°±çŸ¥é“å®ƒæ²¡å¡æ­»
        index = VectorStoreIndex.from_documents(
            documents, 
            show_progress=True, 
            num_workers=4  # i7-13700H å¯ä»¥è®¾ä¸º 4-8ï¼Œæ˜¾è‘—æå‡ CPU å‘é‡åŒ–æ•ˆç‡
        )

        index.storage_context.persist(persist_dir=storage_path)
        print("âœ… ç´¢å¼•æ„å»ºå®Œæˆ")

        # if not os.path.exists(data_path):
        #     raise FileNotFoundError(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼š{data_path}")

        # documents = SimpleDirectoryReader(data_path).load_data()
        # print(f"âœ… å…±è¯»å– {len(documents)} ä¸ªæ–‡æ¡£")

        # print("ğŸ”¨ æ„å»ºå‘é‡ç´¢å¼•ä¸­...")
        # index = VectorStoreIndex.from_documents(documents)

        # print(f"ğŸ’¾ ä¿å­˜ç´¢å¼•åˆ°ï¼š{storage_path}")
        # index.storage_context.persist(persist_dir=storage_path)
        # print("âœ… ç´¢å¼•æ„å»ºå®Œæˆ")

    return index


# ==================== æŸ¥è¯¢å¼•æ“ ====================

def create_query_engine(index):
    """åˆ›å»ºé…ç½®å¥½çš„æŸ¥è¯¢å¼•æ“"""

    print("âš™ï¸  é…ç½®æŸ¥è¯¢å¼•æ“...")

    # åˆå§‹åŒ–é‡æ’åºå™¨(æé«˜æ£€ç´¢ç²¾åº¦)
    reranker = FlagEmbeddingReranker(
        model=Config.RERANKER_MODEL,
        top_n=Config.RERANK_TOP_N
    )

    # åˆ›å»ºæŸ¥è¯¢å¼•æ“
    query_engine = index.as_query_engine(
        similarity_top_k=Config.TOP_K,
        node_postprocessors=[reranker],
        text_qa_template=PromptTemplate(QA_PROMPT_TEMPLATE)
    )

    print("âœ… æŸ¥è¯¢å¼•æ“å°±ç»ª\n")
    return query_engine

def create_chat_engine(index):
    print("âš™ï¸  é…ç½®å¯¹è¯å¼æŸ¥è¯¢å¼•æ“ (å¸¦è®°å¿†åŠŸèƒ½)...")
    memory = ChatMemoryBuffer.from_defaults(token_limit=3000)

    # 1. åˆå§‹åŒ–é‡æ’åºå™¨
    reranker = FlagEmbeddingReranker(
        model=Config.RERANKER_MODEL,
        top_n=Config.RERANK_TOP_N
    )

    # 2. å°† Index è½¬æ¢ä¸º Chat Engine
    # chat_mode="condense_plus_context" æ˜¯æœ€å¼ºå¤§çš„æ¨¡å¼ï¼š
    # å®ƒä¼šå…ˆå‹ç¼©é—®é¢˜ï¼Œå†æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œæœ€åå›ç­”
    # chat_engine = index.as_chat_engine(
    #     chat_mode="condense_plus_context",
    #     similarity_top_k=Config.TOP_K,
    #     node_postprocessors=[reranker],
    #     system_prompt=(
    #         "ä½ æ˜¯ä¸€ä½æ·±æ€ç†Ÿè™‘çš„ç³»ç»Ÿæ¶æ„åˆ†æä¸“å®¶ã€‚"
    #         "ä½ ä¼šç»“åˆå¯¹è¯å†å²å’Œæä¾›çš„å‚è€ƒå†…å®¹æ¥å›ç­”é—®é¢˜ã€‚"
    #         "å¦‚æœç”¨æˆ·è¦æ±‚è§£é‡Šä¹‹å‰çš„å›ç­”ï¼Œè¯·åŠ¡å¿…ç»“åˆä¹‹å‰çš„ä¸Šä¸‹æ–‡ã€‚"
    #     ),
    #     # ä¿æŒä¹‹å‰çš„æç¤ºè¯é£æ ¼
    #     context_prompt=QA_PROMPT_TEMPLATE 
    # )

    chat_engine = index.as_chat_engine(
        chat_mode="condense_plus_context",
        streaming=True,  # å¿…é¡»å¼€å¯æµå¼è¾“å‡ºæ‰èƒ½è¾¹ç­”è¾¹æ˜¾ç¤º
        memory=memory, # å…³é”®ï¼šå¼•å…¥æœ‰ä¸Šé™çš„è®°å¿†
        similarity_top_k=3, # å»ºè®®å°† 10 æ”¹ä¸º 5ï¼Œå‡å°‘ context è´Ÿæ‹…
        #node_postprocessors=[reranker], # ä¹‹å‰çš„é‡æ’åºå™¨
        system_prompt="ä½ æ˜¯ä¸€ä½é‡‘è/æ”¿åŠ¡é›†æˆæ¶æ„ä¸“å®¶...",
        context_prompt=QA_PROMPT_TEMPLATE
    )

    print("âœ… å¯¹è¯å¼•æ“å°±ç»ª\n")
    return chat_engine


# ==================== æˆæœ¬ç»Ÿè®¡ ====================

def print_token_stats(token_counter):
    """æ‰“å°Tokenæ¶ˆè€—å’Œæˆæœ¬ç»Ÿè®¡"""

    prompt_tokens = token_counter.prompt_llm_token_count
    completion_tokens = token_counter.completion_llm_token_count
    total_cost = (
            prompt_tokens * Config.PRICE_INPUT +
            completion_tokens * Config.PRICE_OUTPUT
    )

    print("\n" + "=" * 50)
    print("ğŸ’° æœ¬æ¬¡å¯¹è¯Tokenæ¶ˆè€—ç»Ÿè®¡:")
    print(f"   ğŸ“¥ è¾“å…¥Token:  {prompt_tokens:,}")
    print(f"   ğŸ“¤ è¾“å‡ºToken:  {completion_tokens:,}")
    print(f"   ğŸ’µ æ€»è®¡è´¹ç”¨:   ${total_cost:.6f} USD")
    print("=" * 50)


def print_sources(source_nodes):
    """æ‰“å°æ£€ç´¢åˆ°çš„æ¥æºæ–‡æ¡£"""

    print("\nğŸ“š æ¥æºä¾æ®:")
    print("-" * 50)
    for i, node in enumerate(source_nodes, 1):
        score = node.score
        content = node.node.get_content()[:100].replace('\n', ' ')
        print(f"{i}. [ç›¸å…³åº¦: {score:.4f}]")
        print(f"   {content}...")
        print()


# ==================== ä¸»ç¨‹åº ====================

def start_rag():
    """RAGç³»ç»Ÿä¸»å…¥å£"""

    # è·å–è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_path = os.path.join(current_dir, "..", "data")
    storage_path = os.path.join(current_dir, "..", "storage")

    # åˆå§‹åŒ–ç³»ç»Ÿ
    token_counter = init_settings()

    # åŠ è½½æˆ–åˆ›å»ºç´¢å¼•
    index = get_or_create_index(data_path, storage_path)

    # åˆ›å»ºæŸ¥è¯¢å¼•æ“
    # query_engine = create_query_engine(index)

    # åˆ›å»ºå¯¹è¯å¼æŸ¥è¯¢å¼•æ“
    chat_engine = create_chat_engine(index)

    # äº¤äº’å¼é—®ç­”å¾ªç¯
    print("=" * 50)
    print("ğŸ¤– RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿå·²å¯åŠ¨")
    print("ğŸ’¡ æç¤ºï¼šè¾“å…¥ 'exit' æˆ– 'quit' é€€å‡ºç³»ç»Ÿ")
    print("=" * 50)

    while True:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = input("\nâ“ è¯·è¾“å…¥é—®é¢˜: ").strip()

        # é€€å‡ºæ¡ä»¶
        if user_input.lower() in ['exit', 'quit', 'é€€å‡º']:
            print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
            break

        # è·³è¿‡ç©ºè¾“å…¥
        if not user_input:
            continue

        # é‡ç½®è®¡æ•°å™¨
        token_counter.reset_counts()

        # æ‰§è¡ŒæŸ¥è¯¢
        try:
            # print("\nğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
            # response = query_engine.query(user_input)

            print("\nğŸ” æ­£åœ¨åˆ†æé—®é¢˜å¹¶æ£€ç´¢...")

            response = chat_engine.stream_chat(user_input)

            print("\nâœ¨ å›ç­”:")
            # è¿­ä»£æ‰“å°æµå¼è¾“å‡º
            for token in response.response_gen:
                print(token, end="", flush=True)
            print("\n") # æ¢è¡Œ
            
            # æ³¨æ„è¿™é‡Œæ”¹ç”¨ chat() è€Œä¸æ˜¯ query()
            # response = chat_engine.chat(user_input)
            # è¾“å‡ºç­”æ¡ˆ
            # print(f"\nâœ¨ å›ç­”:\n{response}\n")

            # æ˜¾ç¤ºTokenç»Ÿè®¡
            # print_token_stats(token_counter)

            # æ˜¾ç¤ºæ¥æºæ–‡æ¡£
            # if hasattr(response, 'source_nodes'):
            #     print_sources(response.source_nodes)

        except Exception as e:
            print(f"\nâŒ å‡ºé”™äº†: {str(e)}")


# ==================== ç¨‹åºå…¥å£ ====================

if __name__ == "__main__":
    try:
        start_rag()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç¨‹åºå·²ä¸­æ–­ï¼Œå†è§ï¼")
    except Exception as e:
        print(f"\nâŒ ç³»ç»Ÿé”™è¯¯: {str(e)}")
        raise