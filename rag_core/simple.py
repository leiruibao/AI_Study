"""
ç®€åŒ–ç‰ˆ RAG æ™ºèƒ½é—®ç­”ç³»ç»Ÿ
åŠŸèƒ½ï¼šåŸºäºæ–‡æ¡£çš„é—®ç­” APIï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰
æŠ€æœ¯ï¼šFastAPI + LlamaIndex + DeepSeek
"""

import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    Settings,
    StorageContext,
    load_index_from_storage,
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openai_like import OpenAILike

from llama_index.core.llms import ChatMessage

# ==================== é…ç½®å‚æ•° ====================
# è¿™é‡Œé›†ä¸­ç®¡ç†æ‰€æœ‰é…ç½®ï¼Œæ–¹ä¾¿ä¿®æ”¹

DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")  # ä»ç¯å¢ƒå˜é‡è·å–
DEEPSEEK_MODEL = "deepseek-chat"
EMBED_MODEL = "BAAI/bge-base-zh-v1.5"  # ä¸­æ–‡å‘é‡æ¨¡å‹

# ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œé¿å…è·¯å¾„é—®é¢˜
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.join(BASE_DIR, "data")       # æ”¾æ–‡æ¡£çš„æ–‡ä»¶å¤¹
STORAGE_PATH = os.path.join(BASE_DIR, "storage") # ç´¢å¼•ä¿å­˜ä½ç½®

DATA_BASE_PATH = os.path.join(BASE_DIR, "data")
STORAGE_BASE_PATH = os.path.join(BASE_DIR, "storage")


# ==================== è¯·æ±‚å’Œå“åº”çš„æ•°æ®ç»“æ„ ====================

class QueryRequest(BaseModel):
    """ç”¨æˆ·æé—®çš„è¯·æ±‚æ ¼å¼"""
    query: str           # ç”¨æˆ·çš„é—®é¢˜
    stream: bool = False # æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡ºï¼ˆé»˜è®¤å¦ï¼‰


class QueryResponse(BaseModel):
    """ç³»ç»Ÿå›ç­”çš„å“åº”æ ¼å¼ï¼ˆéæµå¼ï¼‰"""
    answer: str  # AIçš„å›ç­”


# ==================== å…¨å±€å˜é‡ï¼šæ”¹ä¸ºå­—å…¸æ˜ å°„ ====================
# ç”¨æ¥å­˜å‚¨å·²ç»åŠ è½½å¥½çš„ç´¢å¼•ï¼Œé¿å…é‡å¤åŠ è½½

index = None  # å‘é‡ç´¢å¼•ï¼ˆç”¨äºæ£€ç´¢æ–‡æ¡£ï¼‰


# ==================== æ ¸å¿ƒå‡½æ•° ====================

def ensure_directories():
    """
    ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
    å¦‚æœä¸å­˜åœ¨å°±è‡ªåŠ¨åˆ›å»º
    """
    os.makedirs(DATA_PATH, exist_ok=True)
    os.makedirs(STORAGE_PATH, exist_ok=True)
    print(f"ğŸ“‚ æ•°æ®ç›®å½•ï¼š{DATA_PATH}")
    print(f"ğŸ’¾ ç´¢å¼•ç›®å½•ï¼š{STORAGE_PATH}")


def init_system():
    """
    åˆå§‹åŒ–ç³»ç»Ÿï¼šé…ç½®å¤§è¯­è¨€æ¨¡å‹å’Œå‘é‡æ¨¡å‹
    è¿™ä¸ªå‡½æ•°åœ¨æœåŠ¡å¯åŠ¨æ—¶åªè¿è¡Œä¸€æ¬¡
    """
    print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿ...")

    # æ£€æŸ¥ API Key æ˜¯å¦é…ç½®
    if not DEEPSEEK_API_KEY:
        raise ValueError("âŒ è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY")

    # 1. é…ç½®å¤§è¯­è¨€æ¨¡å‹ï¼ˆç”¨äºç”Ÿæˆå›ç­”ï¼‰
    Settings.llm = OpenAILike(
        model=DEEPSEEK_MODEL,
        api_key=DEEPSEEK_API_KEY,
        api_base="https://api.deepseek.com/v1",
        is_chat_model=True,  # å…³é”®ï¼šå‘Šè¯‰ LlamaIndex è¿™æ˜¯ä¸€ä¸ªå¯¹è¯æ¨¡å‹
        temperature=0.3,
        # å¼ºåˆ¶æŒ‡å®šä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼Œé˜²æ­¢å®ƒå› ä¸ºä¸è®¤è¯† deepseek è€ŒæŠ¥é”™
        context_window=64000
    )

    # 2. é…ç½®å‘é‡æ¨¡å‹ï¼ˆç”¨äºç†è§£æ–‡æ¡£å†…å®¹ï¼‰
    Settings.embed_model = HuggingFaceEmbedding(
        model_name=EMBED_MODEL,
        device="cpu"  # ä½¿ç”¨ CPUï¼Œå¦‚æœæœ‰ GPU å¯ä»¥æ”¹æˆ "cuda"
    )

    # 3. è®¾ç½®æ–‡æœ¬åˆ†å—å¤§å°ï¼ˆæŠŠé•¿æ–‡æ¡£åˆ‡æˆå°å—ï¼Œæ–¹ä¾¿æ£€ç´¢ï¼‰
    Settings.chunk_size = 512
    Settings.chunk_overlap = 50  # å—ä¹‹é—´æœ‰é‡å ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤±

    print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\n")


def load_or_create_index():
    """
    åŠ è½½æˆ–åˆ›å»ºç´¢å¼•
    - å¦‚æœå·²æœ‰ç´¢å¼•ï¼šç›´æ¥åŠ è½½
    - å¦‚æœæ²¡æœ‰ï¼šè¯»å–æ–‡æ¡£å¹¶åˆ›å»ºæ–°ç´¢å¼•
    """
    global index

    # æƒ…å†µ1ï¼šå·²ç»æœ‰ç´¢å¼•äº†ï¼Œç›´æ¥åŠ è½½
    if os.path.exists(STORAGE_PATH) and os.listdir(STORAGE_PATH):
        print(f"ğŸ“‚ ä» {STORAGE_PATH} åŠ è½½å·²æœ‰ç´¢å¼•...")
        storage_context = StorageContext.from_defaults(persist_dir=STORAGE_PATH)
        index = load_index_from_storage(storage_context)
        print("âœ… ç´¢å¼•åŠ è½½æˆåŠŸ\n")
        return

    # æƒ…å†µ2ï¼šæ²¡æœ‰ç´¢å¼•ï¼Œéœ€è¦åˆ›å»º
    print(f"ğŸ“š æœªæ‰¾åˆ°ç´¢å¼•ï¼Œå¼€å§‹æ„å»º...")

    # æ£€æŸ¥ data ç›®å½•æ˜¯å¦æœ‰æ–‡ä»¶
    if not os.path.exists(DATA_PATH) or not os.listdir(DATA_PATH):
        print("âš ï¸  data ç›®å½•ä¸ºç©ºï¼Œè¯·å…ˆæ·»åŠ æ–‡æ¡£æ–‡ä»¶")
        print(f"   è¯·å°†æ–‡æ¡£ï¼ˆtxt, pdf, docx ç­‰ï¼‰æ”¾åˆ°ï¼š{DATA_PATH}")
        # åˆ›å»ºç©ºç´¢å¼•ï¼Œé¿å…æŠ¥é”™
        index = VectorStoreIndex.from_documents([])
        return

    print(f"ğŸ“– è¯»å–æ–‡æ¡£ç›®å½•ï¼š{DATA_PATH}")

    # è¯»å–æ‰€æœ‰æ–‡æ¡£ï¼ˆæ”¯æŒ txt, pdf, docx ç­‰æ ¼å¼ï¼‰
    reader = SimpleDirectoryReader(input_dir=DATA_PATH)
    documents = reader.load_data()

    print(f"ğŸ“„ å…±è¯»å– {len(documents)} ä¸ªæ–‡æ¡£")
    print("ğŸ”¨ æ­£åœ¨æ„å»ºç´¢å¼•...")

    # åˆ›å»ºå‘é‡ç´¢å¼•ï¼ˆè¿™ä¸€æ­¥ä¼šæ¯”è¾ƒæ…¢ï¼Œéœ€è¦å¤„ç†æ‰€æœ‰æ–‡æ¡£ï¼‰
    index = VectorStoreIndex.from_documents(
        documents,
        show_progress=True  # æ˜¾ç¤ºè¿›åº¦æ¡
    )

    # ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜ï¼Œä¸‹æ¬¡ç›´æ¥åŠ è½½
    index.storage_context.persist(persist_dir=STORAGE_PATH)
    print("âœ… ç´¢å¼•æ„å»ºå¹¶ä¿å­˜æˆåŠŸ\n")


def get_answer(question: str) -> str:
    """
    æ ¹æ®é—®é¢˜è¿”å›ç­”æ¡ˆï¼ˆéæµå¼ï¼‰

    å·¥ä½œæµç¨‹ï¼š
    1. ç”¨å‘é‡æ£€ç´¢æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
    2. æŠŠç‰‡æ®µå’Œé—®é¢˜ä¸€èµ·å‘ç»™å¤§æ¨¡å‹
    3. å¤§æ¨¡å‹åŸºäºæ–‡æ¡£å†…å®¹ç”Ÿæˆå›ç­”
    """
    if index is None:
        raise ValueError("ç´¢å¼•æœªåŠ è½½")

    print(f"ğŸ” æ­£åœ¨å¤„ç†é—®é¢˜ï¼š{question[:50]}...")

    # åˆ›å»ºæŸ¥è¯¢å¼•æ“ï¼ˆè´Ÿè´£æ£€ç´¢+ç”Ÿæˆå›ç­”ï¼‰
    query_engine = index.as_query_engine(
        similarity_top_k=3  # æ£€ç´¢æœ€ç›¸å…³çš„ 3 ä¸ªæ–‡æ¡£ç‰‡æ®µ
    )

    # æ‰§è¡ŒæŸ¥è¯¢
    response = query_engine.query(question)

    print("âœ… å›ç­”ç”Ÿæˆå®Œæˆ")
    return str(response)


def get_answer_stream(question: str):
    """
    æ ¹æ®é—®é¢˜è¿”å›ç­”æ¡ˆï¼ˆæµå¼è¾“å‡ºï¼‰

    æµå¼è¾“å‡ºçš„å¥½å¤„ï¼š
    - ä¸ç”¨ç­‰å¾…æ‰€æœ‰å†…å®¹ç”Ÿæˆå®Œæ‰çœ‹åˆ°ç»“æœ
    - é€å­—è¾“å‡ºï¼Œä½“éªŒæ›´å¥½
    - é€‚åˆé•¿å›ç­”
    """
    if index is None:
        raise ValueError("ç´¢å¼•æœªåŠ è½½")

    print(f"ğŸ” æ­£åœ¨å¤„ç†æµå¼é—®é¢˜ï¼š{question[:50]}...")

    # åˆ›å»ºæŸ¥è¯¢å¼•æ“ï¼Œå¼€å¯æµå¼æ¨¡å¼
    query_engine = index.as_query_engine(
        similarity_top_k=3,
        streaming=True  # å…³é”®ï¼šå¼€å¯æµå¼è¾“å‡º
    )

    # æ‰§è¡ŒæŸ¥è¯¢ï¼Œè¿”å›æµå¼å“åº”å¯¹è±¡
    streaming_response = query_engine.query(question)

    # è¿”å›ç”Ÿæˆå™¨ï¼Œé€ä¸ªè¾“å‡º token
    return streaming_response.response_gen


# ==================== FastAPI åº”ç”¨ ====================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆæ–°ç‰ˆ FastAPI æ¨èå†™æ³•ï¼‰
    - å¯åŠ¨æ—¶æ‰§è¡Œåˆå§‹åŒ–
    - å…³é—­æ—¶æ‰§è¡Œæ¸…ç†
    """
    # å¯åŠ¨æ—¶æ‰§è¡Œ
    print("ğŸš€ RAG API æœåŠ¡å¯åŠ¨ä¸­...")
    ensure_directories()  # ç¡®ä¿ç›®å½•å­˜åœ¨
    init_system()
    load_or_create_index()
    print("âœ… æœåŠ¡å¯åŠ¨å®Œæˆï¼Œå¯ä»¥å¼€å§‹æé—®äº†ï¼\n")

    yield  # è¿™é‡Œæ˜¯æœåŠ¡è¿è¡ŒæœŸé—´

    # å…³é—­æ—¶æ‰§è¡Œ
    print("ğŸ›‘ æœåŠ¡å…³é—­ä¸­...")


app = FastAPI(
    title="RAG æ™ºèƒ½é—®ç­” API",
    description="åŸºäºæ–‡æ¡£çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰",
    version="1.0.0",
    lifespan=lifespan  # ç»‘å®šç”Ÿå‘½å‘¨æœŸç®¡ç†
)


@app.get("/")
async def root():
    """é¦–é¡µï¼Œæ˜¾ç¤º API ä¿¡æ¯"""
    return {
        "service": "RAG æ™ºèƒ½é—®ç­” API",
        "version": "1.0.0",
        "endpoints": {
            "POST /query": "é—®ç­”æ¥å£ï¼ˆæ”¯æŒæµå¼å’Œéæµå¼ï¼‰",
            "GET /": "API ä¿¡æ¯"
        },
        "usage": {
            "éæµå¼": {"query": "ä½ çš„é—®é¢˜", "stream": False},
            "æµå¼": {"query": "ä½ çš„é—®é¢˜", "stream": True}
        }
    }


@app.post("/query")
async def query(request: QueryRequest):
    """
    é—®ç­”æ¥å£ï¼ˆæ”¯æŒæµå¼å’Œéæµå¼ï¼‰

    ä½¿ç”¨æ–¹æ³•ï¼š
    POST http://localhost:8000/query

    éæµå¼ï¼š
    Body: {"query": "ä½ çš„é—®é¢˜", "stream": false}

    æµå¼ï¼š
    Body: {"query": "ä½ çš„é—®é¢˜", "stream": true}
    """
    try:
        # æ ¹æ® stream å‚æ•°å†³å®šä½¿ç”¨å“ªç§æ¨¡å¼
        if request.stream:
            # æµå¼è¾“å‡ºï¼šé€å­—è¿”å›
            def generate():
                """ç”Ÿæˆå™¨å‡½æ•°ï¼Œç”¨äºæµå¼è¾“å‡º"""
                for token in get_answer_stream(request.query):
                    yield token  # æ¯æ¬¡è¿”å›ä¸€ä¸ªå­—ç¬¦æˆ–è¯

            # è¿”å›æµå¼å“åº”
            return StreamingResponse(
                generate(),
                media_type="text/plain"  # çº¯æ–‡æœ¬æ ¼å¼
            )
        else:
            # éæµå¼è¾“å‡ºï¼šä¸€æ¬¡æ€§è¿”å›å®Œæ•´ç­”æ¡ˆ
            answer = get_answer(request.query)
            return QueryResponse(answer=answer)

    except Exception as e:
        # å¦‚æœå‡ºé”™ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")


@app.post("/mentor-chat")
async def mentor_chat(request: Request):
    data = await request.json()
    q_id = data.get("qId")
    user_query = data.get("query")
    history_data = data.get("history", [])  # å‰ç«¯ä¼ æ¥çš„å¯¹è¯æ•°ç»„

    # 1. å°†å‰ç«¯ä¼ æ¥çš„ history è½¬æ¢ä¸º LlamaIndex éœ€è¦çš„ ChatMessage å¯¹è±¡
    chat_history = []
    for msg in history_data:
        role = "user" if msg["role"] == "user" else "assistant"
        chat_history.append(ChatMessage(role=role, content=msg["content"]))

    # 2. åˆ›å»ºä¸€ä¸ªä¸Šä¸‹æ–‡å¯¹è¯å¼•æ“
    # context_template å¯ä»¥å‘Šè¯‰ AI å®ƒçš„èº«ä»½
    chat_engine = index.as_chat_engine(
        chat_mode="context",
        system_prompt=(
            "ä½ æ˜¯ä¸€åèµ„æ·±çš„æ³•è€ƒå¯¼å¸ˆã€‚ä½ ç°åœ¨çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„æ³•å¾‹æ¡ˆä¾‹èƒŒæ™¯ï¼Œ"
            "å›ç­”å­¦ç”Ÿé’ˆå¯¹è¯¥æ¡ˆä¾‹çš„è¿½é—®ã€‚ä½ çš„å›ç­”è¦ä¸“ä¸šã€ä¸¥è°¨ï¼Œå¹¶å¤šå¼•ç”¨æ¡ˆä¾‹ä¸­çš„äº‹å®ã€‚"
        ),
    )

    # 3. å‘èµ·å¯¹è¯ï¼ˆä¼ å…¥å†å²ï¼ŒAI å°±èƒ½è®°ä½ä¹‹å‰èŠè¿‡ä»€ä¹ˆï¼‰
    response = chat_engine.chat(user_query, chat_history=chat_history)

    return {"answer": response.response}


# ==================== å¯åŠ¨æœåŠ¡ ====================

if __name__ == "__main__":
    import uvicorn

    print("=" * 50)
    print("ğŸš€ å¯åŠ¨ RAG API æœåŠ¡")
    print("=" * 50)

    # å¯åŠ¨ Web æœåŠ¡
    uvicorn.run(
        app,
        host="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
        port=8000,       # ç«¯å£å·
        reload=False     # ç”Ÿäº§ç¯å¢ƒå»ºè®®å…³é—­è‡ªåŠ¨é‡è½½
    )