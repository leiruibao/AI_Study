"""
å¤šå­¦ç§‘å¼ºåŒ–ç‰ˆ RAG æ™ºèƒ½é—®ç­”ç³»ç»Ÿ
åŠŸèƒ½ï¼šæ”¯æŒå­¦ç§‘è·¯ç”±ï¼ˆcrim, civ, javaç­‰ï¼‰ã€è‡ªåŠ¨é¢„çƒ­åŠ è½½ã€æµå¼è¾“å‡º
"""

import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    Settings,
    StorageContext,
    load_index_from_storage,
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openai_like import OpenAILike
from llama_index.core.llms import ChatMessage

# ==================== é…ç½®å‚æ•° ====================

DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")
DEEPSEEK_MODEL = "deepseek-chat"
EMBED_MODEL = "BAAI/bge-base-zh-v1.5"

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_BASE_PATH = os.path.join(BASE_DIR, "data")
STORAGE_BASE_PATH = os.path.join(BASE_DIR, "storage")

# ==================== å…¨å±€çŠ¶æ€ï¼šå­¦ç§‘ç´¢å¼•æ˜ å°„ ====================
# key: å­¦ç§‘æ ‡è¯† (å¦‚ crim), value: å¯¹åº”çš„ VectorStoreIndex å¯¹è±¡
index_map = {}


# ==================== è¯·æ±‚å’Œå“åº”çš„æ•°æ®ç»“æ„ ====================
class QueryRequest(BaseModel):
    query: str
    stream: bool = False
    subject: str = "default"  # æ¥æ”¶æ¥è‡ª Java ç«¯çš„å­¦ç§‘æ ‡è¯† (subject_code)


class QueryResponse(BaseModel):
    answer: str


# ==================== æ ¸å¿ƒé€»è¾‘å‡½æ•° ====================

def init_system():
    """åˆå§‹åŒ– LLM å’Œ Embedding é…ç½®"""
    print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–å…¨å±€ AI é…ç½®...")
    if not DEEPSEEK_API_KEY:
        raise ValueError("âŒ è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY")

    Settings.llm = OpenAILike(
        model=DEEPSEEK_MODEL,
        api_key=DEEPSEEK_API_KEY,
        api_base="https://api.deepseek.com/v1",
        is_chat_model=True,
        temperature=1,
        context_window=64000
    )

    Settings.embed_model = HuggingFaceEmbedding(
        model_name=EMBED_MODEL,
        device="cpu"
    )

    Settings.chunk_size = 512
    Settings.chunk_overlap = 50
    print("âœ… å…¨å±€é…ç½®åˆå§‹åŒ–å®Œæˆ")


def load_index_for_subject(subject_name: str):
    """æ ¹æ®å­¦ç§‘ååŠ è½½æˆ–åˆ›å»ºç´¢å¼• (æ ¸å¿ƒè·¯ç”±é€»è¾‘)"""
    global index_map

    # è·¯å¾„å‡†å¤‡
    subject_data_path = os.path.join(DATA_BASE_PATH, subject_name)
    subject_storage_path = os.path.join(STORAGE_BASE_PATH, subject_name)

    # ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
    os.makedirs(subject_data_path, exist_ok=True) # å¦‚æœç›®å½•é“¾ä¸­æœ‰ä¸å­˜åœ¨çš„æ–‡ä»¶å¤¹åˆ™ä¼šè‡ªåŠ¨åˆ›å»ºï¼ŒTrueä»£è¡¨å­˜åœ¨çš„è¯ä¸ä¼šæŠ¥é”™
    os.makedirs(subject_storage_path, exist_ok=True)

    # å°è¯•åŠ è½½
    if os.path.exists(subject_storage_path) and os.listdir(subject_storage_path):
        print(f"ğŸ“‚ æ­£åœ¨ä»ç£ç›˜åŠ è½½ã€{subject_name}ã€‘ç´¢å¼•...")
        storage_context = StorageContext.from_defaults(persist_dir=subject_storage_path) # StorageContext æ˜¯ LlamaIndex åº“é‡Œçš„ä¸€ä¸ªâ€œé…ç½®ç®¡å®¶â€
        idx = load_index_from_storage(storage_context) # 
    else:
        # å¦‚æœæ²¡æœ‰ç´¢å¼•åˆ™æ„å»º
        print(f"ğŸ“š æ­£åœ¨ä¸ºã€{subject_name}ã€‘æ„å»ºæ–°ç´¢å¼•...")
        if not os.listdir(subject_data_path):
            print(f"âš ï¸  è­¦å‘Š: ã€{subject_name}ã€‘æ•°æ®ç›®å½•ä¸ºç©ºï¼Œåˆ›å»ºç©ºç´¢å¼•")
            idx = VectorStoreIndex.from_documents([])
        else:
            # æœ¬åœ°æ–‡æ¡£å˜æˆ AI èƒ½æ‡‚çš„æ•°æ®åº“
            reader = SimpleDirectoryReader(input_dir=subject_data_path) # æ‰¾æ¬è¿å·¥ã€‚å®ä¾‹åŒ–ä¸€ä¸ªæ‰«æå™¨ï¼Œç„å‡†å­˜æ”¾æ–‡æ¡£çš„æ–‡ä»¶å¤¹
            documents = reader.load_data() # æ¬è´§ä¸Šè½¦ã€‚æŠŠ PDF/Word/TXT ç­‰åŸå§‹æ–‡ä»¶è¯»è¿›å†…å­˜ï¼Œå˜æˆä»£ç èƒ½å¤„ç†çš„é€šç”¨æ ¼å¼ã€‚
            idx = VectorStoreIndex.from_documents(documents, show_progress=True) # åˆ‡ç¢å¹¶ç´¢å¼•ï¼ˆæ ¸å¿ƒæ­¥éª¤ï¼‰ã€‚æŠŠæ–‡æ¡£åˆ‡æˆå°å—ï¼ˆChunkï¼‰ï¼Œè®¡ç®—ç‰¹å¾å€¼ï¼ˆEmbeddingï¼‰ï¼Œåšæˆç±»ä¼¼å­—å…¸çš„â€œç´¢å¼•ä¹¦æ¶â€ã€‚
            idx.storage_context.persist(persist_dir=subject_storage_path) # å­˜å…¥ä»“åº“ã€‚æŠŠå†…å­˜é‡Œåšå¥½çš„ç´¢å¼•ä¿å­˜åˆ°ç¡¬ç›˜ï¼Œä¸‹æ¬¡å¯åŠ¨ç›´æ¥è¯»ï¼Œä¸ç”¨å†é‡å¤å‰ä¸‰æ­¥ã€‚

    index_map[subject_name] = idx
    return idx


def warmup_indexes():
    """å¯åŠ¨é¢„çƒ­ï¼šéå† data æ–‡ä»¶å¤¹åŠ è½½æ‰€æœ‰å­¦ç§‘ç´¢å¼•"""
    print("ğŸ”¥ æ­£åœ¨å¯åŠ¨é¢„çƒ­ï¼Œé¢„åŠ è½½æ‰€æœ‰å­¦ç§‘ç´¢å¼•...")
    if not os.path.exists(DATA_BASE_PATH):
        os.makedirs(DATA_BASE_PATH)
        return

    # è·å–æ‰€æœ‰å­ç›®å½•
    subjects = [d for d in os.listdir(DATA_BASE_PATH)
                if os.path.isdir(os.path.join(DATA_BASE_PATH, d))]

    for sub in subjects:
        try:
            load_index_for_subject(sub)
        except Exception as e:
            print(f"âŒ åŠ è½½å­¦ç§‘ã€{sub}ã€‘å¤±è´¥: {str(e)}")

    print(f"âœ… é¢„çƒ­å®Œæˆï¼Œå·²åŠ è½½å­¦ç§‘: {list(index_map.keys())}")


# ==================== FastAPI ç”Ÿå‘½å‘¨æœŸ ====================

@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸš€ æœåŠ¡å¯åŠ¨ä¸­...")
    init_system()
    warmup_indexes()  # æ‰§è¡Œé¢„çƒ­
    yield
    print("ğŸ›‘ æœåŠ¡å…³é—­ä¸­...")


app = FastAPI(title="RAG å¤šå­¦ç§‘ API", lifespan=lifespan) # lifespanï¼šå‡½æ•°åã€‚ä½ å¯ä»¥èµ·åå« startup_and_shutdownï¼Œä½†åœ¨ FastAPI é‡Œçº¦å®šä¿—æˆå« lifespanï¼ˆç”Ÿå‘½å‘¨æœŸï¼‰ã€‚


# ==================== API æ¥å£ ====================

@app.get("/")
async def root():
    return {"loaded_subjects": list(index_map.keys()), "status": "running"}


@app.post("/query")
async def query(request: QueryRequest):
    """é—®ç­”æ¥å£ï¼šæ”¯æŒå­¦ç§‘è·¯ç”±"""
    # 1. è·å–å­¦ç§‘ç´¢å¼•ï¼ˆå¦‚æœé¢„çƒ­æ²¡åŠ è½½åˆ°ï¼Œè¿™é‡Œä¼šåŠ¨æ€å°è¯•åŠ è½½ï¼‰
    subject = request.subject if request.subject in index_map else "default"
    if subject not in index_map:
        # å°è¯•åŠ¨æ€åŠ è½½ï¼ˆæ¯”å¦‚è¿è¡ŒæœŸé—´æ–°åŠ äº†æ–‡ä»¶å¤¹ï¼‰
        try:
            current_index = load_index_for_subject(request.subject)
        except:
            raise HTTPException(status_code=404, detail=f"å­¦ç§‘åº“ {request.subject} ä¸å­˜åœ¨")
    else:
        current_index = index_map[subject]

    try:
        if request.stream:
            def generate():
                query_engine = current_index.as_query_engine(streaming=True, similarity_top_k=3)
                response = query_engine.query(request.query)
                for token in response.response_gen:
                    yield token

            return StreamingResponse(generate(), media_type="text/plain")
        else:
            query_engine = current_index.as_query_engine(similarity_top_k=3)
            response = query_engine.query(request.query)
            return QueryResponse(answer=str(response))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/mentor-chat")
async def mentor_chat(request: Request):
    """å¯¼å¸ˆå¯¹è¯æ¥å£ï¼šå¸¦å†å²è®°å¿†å’Œå­¦ç§‘èƒŒæ™¯"""
    data = await request.json()
    subject_code = data.get("subject", "default")
    user_query = data.get("query")
    history_data = data.get("history", [])

    # è·å–ç´¢å¼•
    if subject_code not in index_map:
        # å¦‚æœæ²¡åŠ è½½è¿‡åˆ™å°è¯•åŠ è½½
        current_index = load_index_for_subject(subject_code)
    else:
        current_index = index_map[subject_code]

    # è½¬æ¢å†å²æ ¼å¼
    chat_history = []
    for msg in history_data:
        role = "user" if msg["role"] == "user" else "assistant"
        chat_history.append(ChatMessage(role=role, content=msg["content"]))

    # åˆ›å»ºå¯¹è¯å¼•æ“
    chat_engine = current_index.as_chat_engine(
        chat_mode="context",
        system_prompt=(
            f"ä½ æ˜¯ä¸€åèµ„æ·±çš„ã€{subject_code}ã€‘ä¸“å®¶å¯¼å¸ˆã€‚è¯·æ ¹æ®æä¾›çš„æ–‡æ¡£èƒŒæ™¯å›ç­”å­¦ç”Ÿçš„é—®é¢˜ã€‚"
            "å›ç­”è¦ä¸“ä¸šä¸¥è°¨ï¼Œå¤šå¼•ç”¨èƒŒæ™¯ææ–™ä¸­çš„äº‹å®ã€‚"
        )
    )

    response = chat_engine.chat(user_query, chat_history=chat_history)
    return {"answer": response.response}


@app.post("/mentor-chat-stream")
async def mentor_chat_stream(request: Request):
    data = await request.json()
    # è¿™ä¸ª query ç°åœ¨æ˜¯ Java ä¼ è¿‡æ¥çš„â€œè¶…çº§ Promptâ€ï¼Œé‡Œé¢å·²ç»åŒ…å«äº†ä¸“å®¶èº«ä»½å’ŒçŒæ°´åçš„æ¨¡æ¿
    query = data.get("query")
    history_data = data.get("history", [])
    subject = data.get("subject", "default")

    index = index_map.get(subject, index_map.get("default"))

    # 1. è½¬æ¢å†å²æ ¼å¼
    chat_history = []
    # æ’é™¤æ‰æœ€åä¸€æ¡ï¼ˆå› ä¸ºæœ€åä¸€æ¡é€šå¸¸å°±æ˜¯å½“å‰çš„ queryï¼ŒChatEngine ä¼šè‡ªåŠ¨å¤„ç†ï¼‰
    for msg in history_data[:-1]:
        role = "user" if msg["role"] == "user" else "assistant"
        chat_history.append(ChatMessage(role=role, content=msg["content"]))

    # 2. ã€æ ¸å¿ƒæ”¹åŠ¨ã€‘æç®€ç³»ç»Ÿæç¤ºè¯
    # â€œè¯·ä¸¥æ ¼æ‰§è¡Œç”¨æˆ· Prompt ä¸­è®¾å®šçš„ä¸“å®¶è§’è‰²å’Œæ‰¹æ”¹é€»è¾‘ã€‚â€
    minimal_system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªé«˜åº¦ä¸“ä¸šçš„ AI åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹æ–¹æä¾›çš„ã€æ‰¹æ”¹æ ‡å‡†ä¸èº«ä»½è®¾å®šã€‘ï¼Œ"
        "ç»“åˆå‚è€ƒèµ„æ–™ï¼Œä»¥è¯¥ä¸“å®¶çš„å£å»ä¸å­¦ç”Ÿè¿›è¡Œæ·±åº¦å¤ç›˜ã€‚"
    )

    # 3. åˆ›å»ºå¯¹è¯å¼•æ“
    chat_engine = index.as_chat_engine(
        chat_mode="context",
        system_prompt=minimal_system_prompt
    )

    def generate():
        # æ³¨æ„ï¼šè¿™é‡Œçš„ query åŒ…å«äº† Java ä¾§ String.format åçš„æ‰€æœ‰ä¿¡æ¯
        response = chat_engine.stream_chat(query, chat_history=chat_history)
        for token in response.response_gen:
            # è¿™é‡Œçš„ \n\n æ˜¯ SSE åè®®è¦æ±‚çš„æ ¼å¼
            yield f"data: {token}\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream")

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000) # host="0.0.0.0"ï¼šå…¨ç½‘ç›‘å¬ã€‚ä¸ä»…ä»…æœ¬æœº 127.0.0.1 èƒ½è®¿é—®ï¼Œå±€åŸŸç½‘é‡Œçš„ Java åç«¯æˆ–å…¶ä»–æœºå™¨ä¹Ÿèƒ½é€šè¿‡ IP æ‰¾åˆ°å®ƒã€‚