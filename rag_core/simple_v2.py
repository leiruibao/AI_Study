"""
å¤šå­¦ç§‘å¼ºåŒ–ç‰ˆ RAG æ™ºèƒ½é—®ç­”ç³»ç»Ÿ
åŠŸèƒ½ï¼šæ”¯æŒå­¦ç§‘è·¯ç”±ï¼ˆcrim, civ, javaç­‰ï¼‰ã€è‡ªåŠ¨é¢„çƒ­åŠ è½½ã€æµå¼è¾“å‡º
"""

import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    Settings,
    StorageContext,
    load_index_from_storage,
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openai_like import OpenAILike
from llama_index.core.llms import ChatMessage
from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker
from llama_index.core import PromptTemplate

# ==================== é…ç½®å‚æ•° ====================

DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")
DEEPSEEK_MODEL = "deepseek-chat"
EMBED_MODEL = "BAAI/bge-base-zh-v1.5"
RERANKER_MODEL = "BAAI/bge-reranker-base"

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_BASE_PATH = os.path.join(BASE_DIR, "data")
STORAGE_BASE_PATH = os.path.join(BASE_DIR, "storage")

# ==================== å…¨å±€çŠ¶æ€ï¼šå­¦ç§‘ç´¢å¼•æ˜ å°„ ====================
# key: å­¦ç§‘æ ‡è¯† (å¦‚ crim), value: å¯¹åº”çš„ VectorStoreIndex å¯¹è±¡
index_map = {}
reranker = None  # å…¨å±€é‡æ’åºå™¨å®ä¾‹


# ==================== è¯·æ±‚å’Œå“åº”çš„æ•°æ®ç»“æ„ ====================
class QueryRequest(BaseModel):
    query: str
    stream: bool = False
    subject: str = "default"  # æ¥æ”¶æ¥è‡ª Java ç«¯çš„å­¦ç§‘æ ‡è¯† (subject_code)


class QueryResponse(BaseModel):
    answer: str


# ==================== æ ¸å¿ƒé€»è¾‘å‡½æ•° ====================

def init_system():
    """åˆå§‹åŒ– LLM å’Œ Embedding é…ç½®"""
    print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–å…¨å±€ AI é…ç½®...")
    if not DEEPSEEK_API_KEY:
        raise ValueError("âŒ è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY")

    Settings.llm = OpenAILike(
        model=DEEPSEEK_MODEL,
        api_key=DEEPSEEK_API_KEY,
        api_base="https://api.deepseek.com/v1",
        is_chat_model=True,
        temperature=1,
        context_window=64000
    )

    Settings.embed_model = HuggingFaceEmbedding(
        model_name=EMBED_MODEL,
        device="cpu"
    )

    Settings.chunk_size = 512
    Settings.chunk_overlap = 50

    global reranker
    print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–é‡æ’åºæ¨¡å‹...")
    reranker = FlagEmbeddingReranker(
        model=RERANKER_MODEL,
        top_n=3  # é‡æ’åºåæœ€ç»ˆä¿ç•™3æ¡æœ€ç›¸å…³æ–‡æ¡£
    )
    print("âœ… å…¨å±€é…ç½®åˆå§‹åŒ–å®Œæˆ")


def load_index_for_subject(subject_name: str):
    """æ ¹æ®å­¦ç§‘ååŠ è½½æˆ–åˆ›å»ºç´¢å¼• (æ ¸å¿ƒè·¯ç”±é€»è¾‘)"""
    global index_map

    # è·¯å¾„å‡†å¤‡
    subject_data_path = os.path.join(DATA_BASE_PATH, subject_name)
    subject_storage_path = os.path.join(STORAGE_BASE_PATH, subject_name)

    # ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
    os.makedirs(subject_data_path, exist_ok=True) # å¦‚æœç›®å½•é“¾ä¸­æœ‰ä¸å­˜åœ¨çš„æ–‡ä»¶å¤¹åˆ™ä¼šè‡ªåŠ¨åˆ›å»ºï¼ŒTrueä»£è¡¨å­˜åœ¨çš„è¯ä¸ä¼šæŠ¥é”™
    os.makedirs(subject_storage_path, exist_ok=True)

    # å°è¯•åŠ è½½
    if os.path.exists(subject_storage_path) and os.listdir(subject_storage_path):
        print(f"ğŸ“‚ æ­£åœ¨ä»ç£ç›˜åŠ è½½ã€{subject_name}ã€‘ç´¢å¼•...")
        storage_context = StorageContext.from_defaults(persist_dir=subject_storage_path) # StorageContext æ˜¯ LlamaIndex åº“é‡Œçš„ä¸€ä¸ªâ€œé…ç½®ç®¡å®¶â€
        idx = load_index_from_storage(storage_context) # 
    else:
        # å¦‚æœæ²¡æœ‰ç´¢å¼•åˆ™æ„å»º
        print(f"ğŸ“š æ­£åœ¨ä¸ºã€{subject_name}ã€‘æ„å»ºæ–°ç´¢å¼•...")
        if not os.listdir(subject_data_path):
            print(f"âš ï¸  è­¦å‘Š: ã€{subject_name}ã€‘æ•°æ®ç›®å½•ä¸ºç©ºï¼Œåˆ›å»ºç©ºç´¢å¼•")
            idx = VectorStoreIndex.from_documents([])
        else:
            # æœ¬åœ°æ–‡æ¡£å˜æˆ AI èƒ½æ‡‚çš„æ•°æ®åº“
            reader = SimpleDirectoryReader(input_dir=subject_data_path) # æ‰¾æ¬è¿å·¥ã€‚å®ä¾‹åŒ–ä¸€ä¸ªæ‰«æå™¨ï¼Œç„å‡†å­˜æ”¾æ–‡æ¡£çš„æ–‡ä»¶å¤¹
            documents = reader.load_data() # æ¬è´§ä¸Šè½¦ã€‚æŠŠ PDF/Word/TXT ç­‰åŸå§‹æ–‡ä»¶è¯»è¿›å†…å­˜ï¼Œå˜æˆä»£ç èƒ½å¤„ç†çš„é€šç”¨æ ¼å¼ã€‚
            idx = VectorStoreIndex.from_documents(documents, show_progress=True) # åˆ‡ç¢å¹¶ç´¢å¼•ï¼ˆæ ¸å¿ƒæ­¥éª¤ï¼‰ã€‚æŠŠæ–‡æ¡£åˆ‡æˆå°å—ï¼ˆChunkï¼‰ï¼Œè®¡ç®—ç‰¹å¾å€¼ï¼ˆEmbeddingï¼‰ï¼Œåšæˆç±»ä¼¼å­—å…¸çš„â€œç´¢å¼•ä¹¦æ¶â€ã€‚
            idx.storage_context.persist(persist_dir=subject_storage_path) # å­˜å…¥ä»“åº“ã€‚æŠŠå†…å­˜é‡Œåšå¥½çš„ç´¢å¼•ä¿å­˜åˆ°ç¡¬ç›˜ï¼Œä¸‹æ¬¡å¯åŠ¨ç›´æ¥è¯»ï¼Œä¸ç”¨å†é‡å¤å‰ä¸‰æ­¥ã€‚

    index_map[subject_name] = idx
    return idx


def warmup_indexes():
    """å¯åŠ¨é¢„çƒ­ï¼šéå† data æ–‡ä»¶å¤¹åŠ è½½æ‰€æœ‰å­¦ç§‘ç´¢å¼•"""
    print("ğŸ”¥ æ­£åœ¨å¯åŠ¨é¢„çƒ­ï¼Œé¢„åŠ è½½æ‰€æœ‰å­¦ç§‘ç´¢å¼•...")
    if not os.path.exists(DATA_BASE_PATH):
        os.makedirs(DATA_BASE_PATH)
        return

    # è·å–æ‰€æœ‰å­ç›®å½•
    subjects = [d for d in os.listdir(DATA_BASE_PATH)
                if os.path.isdir(os.path.join(DATA_BASE_PATH, d))]

    for sub in subjects:
        try:
            load_index_for_subject(sub)
        except Exception as e:
            print(f"âŒ åŠ è½½å­¦ç§‘ã€{sub}ã€‘å¤±è´¥: {str(e)}")

    print(f"âœ… é¢„çƒ­å®Œæˆï¼Œå·²åŠ è½½å­¦ç§‘: {list(index_map.keys())}")


# ==================== FastAPI ç”Ÿå‘½å‘¨æœŸ ====================

@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸš€ æœåŠ¡å¯åŠ¨ä¸­...")
    init_system()
    warmup_indexes()  # æ‰§è¡Œé¢„çƒ­
    yield
    print("ğŸ›‘ æœåŠ¡å…³é—­ä¸­...")


app = FastAPI(title="LawMaster RAG API", lifespan=lifespan) # lifespanï¼šå‡½æ•°åã€‚ä½ å¯ä»¥èµ·åå« startup_and_shutdownï¼Œä½†åœ¨ FastAPI é‡Œçº¦å®šä¿—æˆå« lifespanï¼ˆç”Ÿå‘½å‘¨æœŸï¼‰ã€‚

# ==================== ç»Ÿä¸€åçš„è¯·æ±‚æ¨¡å‹ ====================
class AIChatRequest(BaseModel):
    query: str
    subject: str = "default"
    history: list = []
    isConcise: bool = False
    stream: bool = True


# ==================== API æ¥å£ ====================

@app.get("/")
async def root():
    return {"loaded_subjects": list(index_map.keys()), "status": "running"}


@app.post("/ai/grade/stream")
async def ai_grade_stream(request: AIChatRequest):
    subject = request.subject if request.subject in index_map else "default"
    current_index = index_map.get(subject)

    # å®šä¹‰ QA æ¨¡æ¿ï¼Œå¼ºåˆ¶è¦æ±‚ Markdown
    qa_prompt_tmpl_str = (
        "ä¸Šä¸‹æ–‡ä¿¡æ¯å¦‚ä¸‹ï¼š\n"
        "---------------------\n"
        "{context_str}\n"
        "---------------------\n"
        "è¯·ç»“åˆä¸Šä¸‹æ–‡å’Œä½ çš„ä¸“ä¸šçŸ¥è¯†å›ç­”é—®é¢˜ï¼š{query_str}\n\n"
        "### å›ç­”è¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š###\n"
        "1. ä½¿ç”¨æ ‡å‡† Markdown æ ¼å¼å›å¤ã€‚\n"
        "2. ä½¿ç”¨ **åŠ ç²—æ ‡é¢˜** åŒºåˆ†æ¨¡å—ã€‚\n"
        "3. å¿…é¡»ä½¿ç”¨ \\n\\n è¿›è¡Œæ¸…æ™°çš„åˆ†æ®µã€‚\n"
        "4. ä½¿ç”¨ - æˆ– â€¢ è¿›è¡Œåˆ—è¡¨æ’ç‰ˆã€‚\n"
        "5. ç»™å‡ºæ˜ç¡®çš„ã€å¾—åˆ†é¢„æµ‹ã€‘ã€‚"
    )
    qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)

    def generate():
        query_engine = current_index.as_query_engine(
            streaming=True,
            similarity_top_k=5,
            node_postprocessors=[reranker],
        )
        # ğŸ”¥ åœ¨è¿™é‡Œæ›´æ–°æ¨¡æ¿ï¼Œè€Œä¸æ˜¯åœ¨ as_query_engine é‡Œä¼  system_prompt
        query_engine.update_prompts(
            {"response_synthesizer:text_qa_template": qa_prompt_tmpl}
        )

        full_response_content = ""
        response = query_engine.query(request.query)
        for token in response.response_gen:
            full_response_content += token
            # 2. è½¬ä¹‰æ¢è¡Œç¬¦ï¼Œç¡®ä¿å®ƒå˜æˆä¸€è¡Œæ™®é€šçš„æ–‡æœ¬
            escaped_token = token.replace("\n", "[NEWLINE_TOKEN]")

            yield f"data:{escaped_token}\n\n"
        print("--- AIæ‰¹æ”¹ï¼šå®Œæ•´ç”Ÿæˆç»“æœ (æœåŠ¡ç«¯è°ƒè¯•) ---")
        print(full_response_content)
        print("--------------------------------")

    return StreamingResponse(generate(), media_type="text/event-stream")

@app.post("/ai/mentor-chat/stream")
async def ai_mentor_chat_stream(request: AIChatRequest):
    """
    ç»Ÿä¸€æ¥å£ï¼šAI å¯¼å¸ˆå¤šè½®å¯¹è¯ï¼ˆæµå¼ï¼‰
    """
    subject = request.subject if request.subject in index_map else "default"
    current_index = index_map.get(subject)

    # è½¬æ¢å†å²è®°å½•æ ¼å¼
    chat_history = [
        ChatMessage(role=("user" if m["role"] == "user" else "assistant"), content=m["content"])
        for m in request.history[:-1]
    ]

    minimal_system_prompt = (
        "è¯·åŠ¡å¿…ä½¿ç”¨æ ‡å‡† Markdown æ ¼å¼å›å¤ï¼Œå¿…é¡»åŒ…å«ï¼š\n"
          "1. ä½¿ç”¨ **åŠ ç²—æ ‡é¢˜**\n"
          "2. ä½¿ç”¨ \n\n è¿›è¡Œåˆ†æ®µ\n"
          "3. ä½¿ç”¨ - æˆ– â€¢ è¿›è¡Œåˆ—è¡¨æ’ç‰ˆã€‚"
    )

    chat_engine = current_index.as_chat_engine(
        chat_mode="context",
        similarity_top_k=10,
        node_postprocessors=[reranker],
        system_prompt=minimal_system_prompt
    )

    def generate():
        response = chat_engine.stream_chat(request.query, chat_history=chat_history)
        full_response_content = ""
        for token in response.response_gen:
            full_response_content += token
            yield f"data: {token}\n\n"
        print("--- å¯¼å¸ˆå¯¹è¯ï¼šå®Œæ•´ç”Ÿæˆç»“æœ (æœåŠ¡ç«¯è°ƒè¯•) ---")
        print(full_response_content)
        print("--------------------------------")

    return StreamingResponse(generate(), media_type="text/event-stream")

@app.post("/mentor-chat-stream")
async def mentor_chat_stream(request: Request):
    data = await request.json()
    # è¿™ä¸ª query ç°åœ¨æ˜¯ Java ä¼ è¿‡æ¥çš„â€œè¶…çº§ Promptâ€ï¼Œé‡Œé¢å·²ç»åŒ…å«äº†ä¸“å®¶èº«ä»½å’ŒçŒæ°´åçš„æ¨¡æ¿
    query = data.get("query")
    history_data = data.get("history", [])
    subject = data.get("subject", "default")

    index = index_map.get(subject, index_map.get("default"))

    # 1. è½¬æ¢å†å²æ ¼å¼
    chat_history = []
    # æ’é™¤æ‰æœ€åä¸€æ¡ï¼ˆå› ä¸ºæœ€åä¸€æ¡é€šå¸¸å°±æ˜¯å½“å‰çš„ queryï¼ŒChatEngine ä¼šè‡ªåŠ¨å¤„ç†ï¼‰
    for msg in history_data[:-1]:
        role = "user" if msg["role"] == "user" else "assistant"
        chat_history.append(ChatMessage(role=role, content=msg["content"]))

    # 2. ã€æ ¸å¿ƒæ”¹åŠ¨ã€‘æç®€ç³»ç»Ÿæç¤ºè¯
    minimal_system_prompt = (
        "è¯·åŠ¡å¿…ä½¿ç”¨æ ‡å‡† Markdown æ ¼å¼å›å¤ï¼Œå¿…é¡»åŒ…å«ï¼š\n"
          "1. ä½¿ç”¨ **åŠ ç²—æ ‡é¢˜**\n"
          "2. ä½¿ç”¨ \n\n è¿›è¡Œåˆ†æ®µ\n"
          "3. ä½¿ç”¨ - æˆ– â€¢ è¿›è¡Œåˆ—è¡¨æ’ç‰ˆã€‚"
    )

    # 3. åˆ›å»ºå¯¹è¯å¼•æ“
    chat_engine = index.as_chat_engine(
        chat_mode="context",
        similarity_top_k=10,
        node_postprocessors=[reranker],
        system_prompt=minimal_system_prompt
    )

    def generate():
        # æ³¨æ„ï¼šè¿™é‡Œçš„ query åŒ…å«äº† Java ä¾§ String.format åçš„æ‰€æœ‰ä¿¡æ¯
        response = chat_engine.stream_chat(query, chat_history=chat_history)
        full_response_content = ""
        for token in response.response_gen:
            full_response_content += token
            yield f"data: {token}\n\n"
        print("--- å®Œæ•´ç”Ÿæˆç»“æœ (æœåŠ¡ç«¯è°ƒè¯•) ---")
        print(full_response_content)
        print("--------------------------------")

    return StreamingResponse(generate(), media_type="text/event-stream")


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000) # host="0.0.0.0"ï¼šå…¨ç½‘ç›‘å¬ã€‚ä¸ä»…ä»…æœ¬æœº 127.0.0.1 èƒ½è®¿é—®ï¼Œå±€åŸŸç½‘é‡Œçš„ Java åç«¯æˆ–å…¶ä»–æœºå™¨ä¹Ÿèƒ½é€šè¿‡ IP æ‰¾åˆ°å®ƒã€‚